{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision\nimport torch.optim as optim\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize CIFAR-10 images to 224x224 to match DeiT input size\n    transforms.ToTensor(),\n    # Normalize using ImageNet mean and std\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Download and load CIFAR-10 training dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n\n# Download and load CIFAR-10 test dataset\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DeiTForImageClassificationWithTeacher\n\nbaseline_model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\nbaseline_model = baseline_model.to(device)\npruned_model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\npruned_model = pruned_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model.distillation_classifier = torch.nn.Linear(in_features=pruned_model.distillation_classifier.in_features, out_features=10)\npruned_model.cls_classifier = torch.nn.Linear(in_features=pruned_model.cls_classifier.in_features, out_features=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model = pruned_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pruned_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def initialize_mask_MLP(layer):\n    mask = torch.ones_like(layer.weight.data)\n    layer.register_buffer('pruning_mask', mask)\n    print(\"Initialized mask for MLP layer\")\n\ndef initialize_mask_attn_layer(layer):\n    for name in ['query', 'key', 'value']:\n        linear_layer = getattr(layer, name, None)\n        if linear_layer is not None:\n            weight_matrix = linear_layer.weight\n            mask = torch.ones_like(weight_matrix.data)\n            layer.register_buffer(f'{name}_pruning_mask', mask)\n            print(f\"Initialized mask for self-attention {name}\")\n\ndef initialize_masks_for_model(model):\n    for name, module in model.named_modules():\n        if 'cls_classifier' in name or 'distillation_classifier' in name:\n            continue  # Skip initializing masks for these specific layers\n\n        if isinstance(module, torch.nn.Linear):\n            initialize_mask_MLP(module)\n        elif 'DeiTSelfAttention' in str(type(module)) or 'DeiTAttention' in str(type(module)):  # A more flexible check\n            # This assumes the self-attention module directly contains the query, key, value attributes\n            if hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n                initialize_mask_attn_layer(module)\n                \n                \ninitialize_masks_for_model(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_and_view_masks(model):\n    for name, module in model.named_modules():\n        # Check for MLP layer masks\n        if hasattr(module, 'pruning_mask'):\n            print(f\"{name} has a pruning mask.\")\n            # Print the mask to verify its contents\n            print(module.pruning_mask)\n\n        # Check for self-attention layer masks\n        for attn_part in ['query', 'key', 'value']:\n            mask_name = f'{attn_part}_pruning_mask'\n            if hasattr(module, mask_name):\n                print(f\"{name} has a {mask_name}.\")\n                # Print the mask to verify its contents\n                print(getattr(module, mask_name))\n\n# Example usage, assuming your model variable is named `model` and you have already called initialize_masks_for_model(model):\ncheck_and_view_masks(pruned_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def copy_weights_to_mask(layer):\n    # Handle MLP layer\n    if hasattr(layer, 'pruning_mask'):\n        layer.pruning_mask.data = torch.clone(layer.weight.data)\n    \n    # Handle self-attention layer\n    for name in ['query', 'key', 'value']:\n        mask_name = f'{name}_pruning_mask'\n        if hasattr(layer, mask_name):\n            attn_weight = getattr(layer, name).weight\n            attn_mask = getattr(layer, mask_name)\n            attn_mask.data = torch.clone(attn_weight.data)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def view_weights_and_masks(model):\n    for name, module in model.named_modules():\n        # Check for layers with weights (typically MLP layers)\n        if hasattr(module, 'weight') and hasattr(module, 'pruning_mask'):\n            print(f\"Layer: {name}\")\n            print(\"Weights:\")\n            print(module.weight.data)\n            print(\"Mask:\")\n            print(module.pruning_mask.data)\n            print(\"-----------------------------------\")\n\n        # Additionally, check for self-attention layers if they have separate masks\n        if hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            for part in ['query', 'key', 'value']:\n                if hasattr(module, f'{part}') and hasattr(module, f'{part}_pruning_mask'):\n                    attn_component = getattr(module, part)\n                    mask = getattr(module, f'{part}_pruning_mask')\n                    print(f\"Layer: {name} - {part.capitalize()}\")\n                    print(\"Weights:\")\n                    print(attn_component.weight.data)\n                    print(\"Mask:\")\n                    print(mask.data)\n                    print(\"-----------------------------------\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_weights_and_masks(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_copy_weights_to_masks(model):\n    for name, module in model.named_modules():\n        # Apply to MLP layers directly\n        if hasattr(module, 'pruning_mask'):\n            copy_weights_to_mask(module)\n        \n        # For self-attention layers in DeiT, the attention mechanism is wrapped within another module.\n        # Thus, we directly check if the module contains self-attention components.\n        for attn_part in ['query', 'key', 'value']:\n            if hasattr(module, f'{attn_part}'):\n                copy_weights_to_mask(module)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply_copy_weights_to_masks(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_weights_and_masks(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def check_and_compare_weights_with_masks(model):\n#     for name, module in model.named_modules():\n#         # Check and compare for MLP layer\n#         if hasattr(module, 'pruning_mask'):\n#             print(f\"{name} (MLP layer) - Mask copied correctly: \", \n#                   torch.equal(module.pruning_mask.data, module.weight.data))\n\n#         # Check and compare for self-attention layer components\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 weight = getattr(module, attn_part).weight\n#                 mask = getattr(module, mask_name)\n#                 print(f\"{name} (Self-attention {attn_part}) - Mask copied correctly: \",\n#                       torch.equal(mask.data, weight.data))\n\n# # After performing the copy operation with apply_copy_weights_to_masks(model):\n# # apply_copy_weights_to_masks(model)\n# # Now, check and compare the masks with the weights:\n# check_and_compare_weights_with_masks(pruned_model)\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_z_score_normalization(mask):\n    print(\"Z score called\")\n    mean = torch.mean(mask)\n    std = torch.std(mask)\n    normalized_mask = (mask - mean) / std\n    print(normalized_mask)\n    return normalized_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def binarize_mask(mask, pruning_ratio):\n    print(\"Binarize called\")\n    # Flatten the mask to simplify thresholding\n    flat_mask = mask.view(-1)\n    # Calculate the number of weights to keep\n    num_weights_to_keep = int((1 - pruning_ratio) * flat_mask.numel())\n    # Use torch.topk to get the threshold value\n    threshold_value, _ = torch.topk(flat_mask.abs(), num_weights_to_keep, largest=True)\n    min_value_to_keep = threshold_value[-1]\n    # Binarize the mask\n    binarized_mask = torch.where(flat_mask.abs() >= min_value_to_keep, torch.tensor(1.0, device=mask.device), torch.tensor(0.0, device=mask.device))\n    print(binarized_mask)\n    return binarized_mask.view_as(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaptivePruning:\n    def __init__(self, prev_val_acc, delta=0.01, performance_threshold=0.02):\n        self.delta = delta\n        self.performance_threshold = performance_threshold\n        self.previous_accuracy = prev_val_acc\n\n    def adjust_pruning_rate(self, current_accuracy, current_pruning_rate):\n        accuracy_change = current_accuracy - self.previous_accuracy\n\n        # Adjust the pruning rate based on the accuracy change\n        if accuracy_change < -self.performance_threshold:\n            adjusted_pruning_rate = max(current_pruning_rate - self.delta, 0)\n        elif accuracy_change >= self.performance_threshold:\n            adjusted_pruning_rate = min(current_pruning_rate + self.delta, 1)\n        else:\n            adjusted_pruning_rate = current_pruning_rate\n\n        # Update the previous accuracy for the next iteration\n        self.previous_accuracy = current_accuracy\n\n        return adjusted_pruning_rate\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef combined_loss(output, target, model, lambda_sparsity=1e-4):\n    # Cross Entropy Loss\n    ce_loss = F.cross_entropy(output, target)\n    \n    # Sparsity Loss (L1 norm of model weights)\n    sparsity_loss = 0\n    for param in model.parameters():\n        sparsity_loss += torch.sum(torch.abs(param))\n    \n    # Combined Loss\n    combined_loss = ce_loss + lambda_sparsity*sparsity_loss\n    \n    return combined_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_pruning_rate(epoch, max_epoch, initial_rate, final_rate):\n    # Linear scheduling from initial_rate to final_rate\n    current_rate = initial_rate + (final_rate - initial_rate) * (epoch / max_epoch)\n    return current_rate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_mask_MLP(layer):\n    if hasattr(layer, 'pruning_mask'):\n        # Ensure the mask is on the same device as the layer weights\n        mask = layer.pruning_mask.to(layer.weight.device)\n        # Apply the mask by element-wise multiplication\n        masked_weights = layer.weight.data.mul(mask)\n        # Explicitly assign the masked weights back to the layer's weight attribute\n        layer.weight.data = masked_weights\n        if layer.bias is not None and hasattr(layer, 'bias_mask'):\n            bias_mask = layer.bias_mask.to(layer.bias.device)\n            masked_bias = layer.bias.data.mul(bias_mask)\n            layer.bias.data = masked_bias\n        print(\"Applied pruning mask to MLP layer\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_mask_attn_layer(layer):\n    # Iterate through each component of the self-attention mechanism\n    for name in ['query', 'key', 'value']:\n        # Access the linear layer for query, key, value\n        attn_component = getattr(layer, name, None)\n        if attn_component is not None:\n            # Access the weight tensor of the component\n            weight_matrix = attn_component.weight\n            # Access the corresponding mask\n            mask_name = f'{name}_pruning_mask'\n            mask = getattr(layer, mask_name, None)\n            if mask is not None:\n                # Ensure the mask is on the same device as the weights\n                mask = mask.to(weight_matrix.device)\n                # Apply the mask by element-wise multiplication\n                weight_matrix.data.mul_(mask)\n                print(f\"Applied mask to {name} weights in self-attention layer\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_masks_for_model(model):\n    for name, module in model.named_modules():\n        print(\"Going to modules\")\n\n        if isinstance(module, torch.nn.Linear):\n            print(\"going to MLP\")\n            apply_mask_MLP(module)\n        # Adjust the condition to check for self-attention layers more accurately\n        elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            print(\"going to self attn\")\n            apply_mask_attn_layer(module)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_binarization_accuracy(model, pruning_ratio):\n    \n    binarization_accuracy = {}\n\n    for name, module in model.named_modules():\n        if 'cls_classifier' in name or 'distillation_classifier' in name:\n            continue  # Skip initializing masks for these specific layers\n\n        masks_to_check = []\n\n        # For MLP layers, add the single pruning mask to the list\n        if hasattr(module, 'pruning_mask'):\n            masks_to_check.append(module.pruning_mask)\n\n        # For self-attention layers, add query, key, and value masks to the list\n        for attn_part in ['query', 'key', 'value']:\n            mask_name = f'{attn_part}_pruning_mask'\n            if hasattr(module, mask_name):\n                masks_to_check.append(getattr(module, mask_name))\n\n        # Calculate and compare sparsity for each mask\n        for mask in masks_to_check:\n            total_elements = mask.numel()\n            zero_elements = total_elements - torch.count_nonzero(mask)\n            actual_sparsity = zero_elements.float() / total_elements\n            expected_sparsity = pruning_ratio\n\n            # Store the results\n            binarization_accuracy[name] = (expected_sparsity, actual_sparsity)\n\n            # Optionally, print out the results for each mask\n            print(f\"{name}: Expected Sparsity: {expected_sparsity:.3f}, Actual Sparsity: {actual_sparsity:.3f}\")\n\n    return binarization_accuracy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#apply_masks_for_model(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_weights_and_masks(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def view_weights_of_layers(model):\n#     for name, module in model.named_modules():\n#         # Check for MLP layer weights\n#         if isinstance(module, torch.nn.Linear):\n#             print(f\"Viewing weights for MLP layer: {name}\")\n#             print(module.weight.data)\n\n#         # Check for self-attention layer weights\n#         # This assumes your model's self-attention mechanism follows a specific naming convention or structure\n#         elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n#             print(f\"Viewing weights for Self-Attention layer: {name}\")\n#             for attn_part in ['query', 'key', 'value']:\n#                 weight_matrix = getattr(module, attn_part).weight.data\n#                 print(f\"Weights for {attn_part}:\")\n#                 print(weight_matrix)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view_weights_of_layers(pruned_model)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def check_weights_squared(model):\n#     for name, module in model.named_modules():\n#         # Check for MLP layers and their masks\n#         if isinstance(module, torch.nn.Linear) and hasattr(module, 'pruning_mask'):\n#             original_mask = module.pruning_mask.data\n#             squared_mask = original_mask ** 2\n#             current_weights = module.weight.data\n            \n#             # Compare the squared mask with the current weights\n#             if torch.allclose(squared_mask, current_weights, rtol=1e-05, atol=1e-08):\n#                 print(f\"{name}: Weights are squared of the original mask values.\")\n#             else:\n#                 print(f\"{name}: Weights are NOT squared of the original mask values.\")\n                \n#         # Check for self-attention layer weights and their masks\n#         elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n#             for part in ['query', 'key', 'value']:\n#                 linear_layer = getattr(module, part)\n#                 if hasattr(module, f'{part}_pruning_mask'):\n#                     original_mask = getattr(module, f'{part}_pruning_mask').data\n#                     squared_mask = original_mask ** 2\n#                     current_weights = linear_layer.weight.data\n                    \n#                     # Compare the squared mask with the current weights\n#                     if torch.allclose(squared_mask, current_weights, rtol=1e-05, atol=1e-08):\n#                         print(f\"{name} {part}: Weights are squared of the original mask values.\")\n#                     else:\n#                         print(f\"{name} {part}: Weights are NOT squared of the original mask values.\")\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check_weights_squared(pruned_model)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import inspect\n# from transformers import DeiTForImageClassificationWithTeacher\n\n# # Attempt to print the forward method source code\n# print(inspect.getsource(DeiTForImageClassificationWithTeacher.forward))","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torchvision.utils as utils\n# class_labels = train_dataset.classes\n# class_labels","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for data, targets in train_loader:\n#     print(f'{data.shape} {targets.shape}')\n    \n#     num_images_to_display = 4\n#     grid_images = utils.make_grid(data[:num_images_to_display], nrow=num_images_to_display)\n\n#     # Move the tensor to CPU and convert it to a NumPy array for visualization\n#     grid_images = grid_images.cpu().numpy().transpose((1, 2, 0))\n#     print([class_labels[x] for x in targets[:num_images_to_display]])\n#     # Display the images\n#     plt.imshow(grid_images)\n#     plt.title('Batch of Images')\n#     plt.axis('off')\n#     plt.show()\n#     break","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with torch.no_grad():\n#     for data, targets in train_loader:\n#         input_data = data\n#         break\n#     outputs = x_model(input_data)\n#     logits = outputs[\"logits\"]\n#     logits.shape","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for epoch in epochs:\n#     for data, targets in train_loader:\n#         outputs = model(data)\n        ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(epoch, model, train_loader, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n    for batch_idx, (inputs, targets) in progress_bar:\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = combined_loss(outputs.logits, targets, model)  # Use the defined combined_loss function\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.logits.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        progress_bar.set_description(f'Epoch {epoch} Loss: {running_loss/(batch_idx+1):.3f} Acc: {100.*correct/total:.3f}%')\n    \n    return running_loss / len(train_loader), 100.*correct / total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, test_loader, device):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.logits.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    \n    acc = 100.*correct / total\n    print(f'Validation Accuracy: {acc:.3f}%')\n    return acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def unified_pruning(epoch, model, train_loader, optimizer, device, initial_pruning_rate, performance_threshold, max_epoch):\n    \n    \n#     apply_copy_weights_to_masks(pruned_model)\n    \n#     for name, module in model.named_modules():\n    \n#         if hasattr(module, 'pruning_mask'):\n#             print(\"z score for MLP unified pruning\")\n#             module.pruning_mask.data = apply_z_score_normalization(module.pruning_mask.data)\n#         elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n#             print(\"going to self attn\")\n#             for name in ['query', 'key', 'value']:\n#                 # Access the linear layer for query, key, value\n#                 attn_component = getattr(module, name, None)\n#                 if attn_component is not None:\n#                     mask_name = f'{name}_pruning_mask'\n#                     mask = getattr(module, mask_name, None)\n#                     if mask is not None:\n#                         zscore = apply_z_score_normalization(mask)\n#                         setattr(module, mask_name, zscore)\n            \n# #         for attn_part in ['query', 'key', 'value']:\n# #             mask_name = f'{attn_part}_pruning_mask'\n# #             if hasattr(module, mask_name):\n# #                 mask = getattr(module, mask_name)\n# #                 print(\"z score for attn unified pruning\")\n# #                 setattr(module, mask_name, apply_z_score_normalization(mask))\n                \n#     current_accuracy = validate(model, test_loader, device) \n#     adaptive_pruning = AdaptivePruning(prev_val_acc=current_accuracy, initial_pruning_rate=initial_pruning_rate, delta=0.01, performance_threshold=performance_threshold)\n#     current_pruning_rate = adaptive_pruning.adjust_pruning_rate(current_accuracy)\n    \n#     for name, module in model.named_modules():\n        \n#         if hasattr(module, 'pruning_mask'):\n#             print(\"brinarize for MLP unified pruning\")\n#             module.pruning_mask.data = binarize_mask(module.pruning_mask.data, current_pruning_rate)\n#         elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n#             print(\"going to self attn\")\n#             for name in ['query', 'key', 'value']:\n#                 # Access the linear layer for query, key, value\n#                 attn_component = getattr(module, name, None)\n#                 if attn_component is not None:\n#                     mask_name = f'{name}_pruning_mask'\n#                     mask = getattr(module, mask_name, None)\n#                     if mask is not None:\n#                         binarized = binarize_mask(mask, current_pruning_rate)\n#                         setattr(module, mask_name, binarized)\n            \n# #         for attn_part in ['query', 'key', 'value']:\n# #             mask_name = f'{attn_part}_pruning_mask'\n# #             if hasattr(module, mask_name):\n# #                 mask = getattr(module, mask_name)\n# #                 print(\"brinarize for attn unified pruning\")\n# #                 setattr(module, mask_name, binarize_mask(mask, current_pruning_rate))\n    \n#     binarization_results = check_binarization_accuracy(model, current_pruning_rate)\n#     apply_masks_for_model(model)\n#     check_pruning_effectiveness(model, current_pruning_rate)\n#     freeze_pruned_weights(model)\n    \n#     train_loss, train_acc = train_one_epoch(epoch, model, train_loader, optimizer, device)\n#     return train_loss, train_acc\n# # epoch_loss, epoch_acc = unified_pruning(pruned_model, current_epoch, total_epochs, train_loader, optimizer, device, initial_rate, final_rate)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unified_progressive_pruning(model, epoch, max_epoch, train_loader, optimizer, device, initial_pruning_rate, final_pruning_rate):\n    \n    \n    train_loss, train_acc = train_one_epoch(epoch, model, train_loader, optimizer, device)\n\n    \n    print(\"copying weights for prog pruning\")\n    apply_copy_weights_to_masks(model)\n    \n    \n    for name, module in model.named_modules():\n        \n        if hasattr(module, 'pruning_mask'):\n            print(\"z score for MLP progressive pruning\")\n            module.pruning_mask.data = apply_z_score_normalization(module.pruning_mask.data)\n        elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            print(\"going to self attn\")\n            for name in ['query', 'key', 'value']:\n                # Access the linear layer for query, key, value\n                attn_component = getattr(module, name, None)\n                if attn_component is not None:\n                    mask_name = f'{name}_pruning_mask'\n                    mask = getattr(module, mask_name, None)\n                    if mask is not None:\n                        zscore = apply_z_score_normalization(mask)\n                        setattr(module, mask_name, zscore)\n            \n                            \n    print(\"updating weights for prog pruning\")\n    pruning_rate = update_pruning_rate(epoch, max_epoch, initial_pruning_rate, final_pruning_rate)\n    current_accuracy = validate(model, test_loader, device)\n    pruning_rate = adaptive_pruning.adjust_pruning_rate(current_accuracy,pruning_rate)\n\n\n\n    print(\"Binarizing masks for prog pruning\")\n    for name, module in model.named_modules():\n        \n        if hasattr(module, 'pruning_mask'):\n            print(\"brinarize for MLP unified pruning\")\n            module.pruning_mask.data = binarize_mask(module.pruning_mask.data, pruning_rate)\n        elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            print(\"going to self attn\")\n            for name in ['query', 'key', 'value']:\n                # Access the linear layer for query, key, value\n                attn_component = getattr(module, name, None)\n                if attn_component is not None:\n                    mask_name = f'{name}_pruning_mask'\n                    mask = getattr(module, mask_name, None)\n                    if mask is not None:\n                        binarized = binarize_mask(mask, pruning_rate)\n                        setattr(module, mask_name, binarized)\n           \n                \n    print(\"applying for progressive pruning\")\n    \n    binarization_results = check_binarization_accuracy(model, pruning_rate)\n    apply_masks_for_model(model)\n    check_pruning_effectiveness(model, pruning_rate)\n    freeze_pruned_weights(model)\n    \n    return current_accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-02-06T01:15:19.263803Z","iopub.execute_input":"2024-02-06T01:15:19.264474Z","iopub.status.idle":"2024-02-06T01:15:19.284329Z","shell.execute_reply.started":"2024-02-06T01:15:19.264442Z","shell.execute_reply":"2024-02-06T01:15:19.283375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze_pruned_weights(model):\n    for name, module in model.named_modules():\n        if hasattr(module, 'pruning_mask'):\n            # Freeze weights for MLP layers\n            def mlp_hook(grad, mask=module.pruning_mask):\n                return grad * mask\n\n            module.weight.register_hook(mlp_hook)\n            print(f\"Freezing pruned weights in {name} (MLP layer)\")\n\n        elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            # Handle self-attention layers' query, key, value\n            for attn_part in ['query', 'key', 'value']:\n                linear_layer = getattr(module, attn_part, None)\n                if linear_layer is not None:\n                    mask_name = f'{attn_part}_pruning_mask'\n                    mask = getattr(module, mask_name, None)\n                    if mask is not None:\n                        def attn_hook(grad, mask=mask):\n                            return grad * mask\n\n                        # Applying hook to the weight parameter of the linear layers within the self-attention mechanism\n                        linear_layer.weight.register_hook(attn_hook)\n                        print(f\"Freezing pruned weights in {name}.{attn_part} (Self-attention component)\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_pruning_effectiveness(model, desired_pruning_ratio):\n    for name, module in model.named_modules():\n        # Check for MLP layers\n        if isinstance(module, torch.nn.Linear):\n            mask = getattr(module, 'pruning_mask', None)\n            if mask is not None:\n                actual_sparsity = torch.mean((mask == 0).float()).item()\n                print(f\"MLP Layer {name}: Desired sparsity = {desired_pruning_ratio}, Actual sparsity = {actual_sparsity}\")\n                if actual_sparsity < desired_pruning_ratio:\n                    print(f\"Warning: {name} layer is under-pruned.\")\n                elif actual_sparsity > desired_pruning_ratio:\n                    print(f\"Warning: {name} layer is over-pruned.\")\n        # Check for self-attention layers\n        elif hasattr(module, 'query') and hasattr(module, 'key') and hasattr(module, 'value'):\n            for attn_name in ['query', 'key', 'value']:\n                attn_component = getattr(module, attn_name, None)\n                if attn_component is not None:\n                    mask_name = f'{attn_name}_pruning_mask'\n                    mask = getattr(module, mask_name, None)\n                    if mask is not None:\n                        actual_sparsity = torch.mean((mask == 0).float()).item()\n                        print(f\"Self-attention {attn_name} in {name}: Desired sparsity = {desired_pruning_ratio}, Actual sparsity = {actual_sparsity}\")\n                        if actual_sparsity < desired_pruning_ratio:\n                            print(f\"Warning: {name} {attn_name} is under-pruned.\")\n                        elif actual_sparsity > desired_pruning_ratio:\n                            print(f\"Warning: {name} {attn_name} is over-pruned.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters and settings\ninitial_pruning_rate = 0.1 \nfinal_pruning_rate = 0.8   \nperformance_threshold = 0.02  \nlambda_sparsity = 1e-4  # Sparsity weighting factor for combined loss\noptimizer = optim.Adam(pruned_model.parameters(), lr=0.001)\nmax_epoch = 16","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiating Training\nfor epoch in range(1, 17):  # 1 to 16 epochs\n    if epoch in [1]:\n        prev_val_acc= validate(pruned_model, test_loader, device)\n        \n    if epoch in [15, 16]:\n        # Train the model normally\n        print(f\"Epoch {epoch}: Normal Training\")\n        train_loss, train_acc = train_one_epoch(epoch, pruned_model, train_loader, optimizer, device)\n        print(f\"Training Loss: {train_loss}, Training Accuracy: {train_acc}\")\n        val_acc = validate(pruned_model, test_loader, device)\n        print(f\"Validation Accuracy: {val_acc}%\")\n    else:\n        # Apply Progressive Pruning\n        adaptive_pruning = AdaptivePruning(prev_val_acc)\n        print(f\"Epoch {epoch}: Applying Progressive Pruning\")\n        prev_val_acc= unified_progressive_pruning(pruned_model, epoch, max_epoch, train_loader, optimizer, device, initial_pruning_rate, final_pruning_rate)\n        print(f\"Validation Accuracy: {prev_val_acc}%\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"view_weights_and_masks(pruned_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install transformers\n# import torch\n# from torchvision import datasets, transforms\n# import matplotlib.pyplot as plt\n# import numpy as np\n# import torchvision\n# import torch.optim as optim\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device\n# transform = transforms.Compose([\n#     transforms.Resize((224, 224)),  # Resize CIFAR-10 images to 224x224 to match DeiT input size\n#     transforms.ToTensor(),\n#     # Normalize using ImageNet mean and std\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n# ])\n\n# # Download and load CIFAR-10 training dataset\n# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n\n# # Download and load CIFAR-10 test dataset\n# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# no_of_batches = len(train_loader)\n# no_of_batches, 50000/64\n\n# from transformers import DeiTForImageClassificationWithTeacher\n\n# baseline_model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\n# baseline_model = baseline_model.to(device)\n# pruned_model = DeiTForImageClassificationWithTeacher.from_pretrained('facebook/deit-base-distilled-patch16-224')\n# pruned_model = pruned_model.to(device)\n\n# pruned_model\n\n# pruned_model.distillation_classifier = torch.nn.Linear(in_features=pruned_model.distillation_classifier.in_features, out_features=10)\n# pruned_model.cls_classifier = torch.nn.Linear(in_features=pruned_model.cls_classifier.in_features, out_features=10)\n\n# pruned_model\n\n# num_output_features = pruned_model.distillation_classifier.out_features\n# print(\"Number of output features:\", num_output_features)\n\n# pruned_model = pruned_model.to(device)\n\n# pruned_model\n\n# def initialize_mask_MLP(layer):\n#     mask = torch.ones_like(layer.weight.data)\n#     layer.register_buffer('pruning_mask', mask)\n    \n# def initialize_mask_attn_layer(layer):\n#     for name in ['query', 'key', 'value']:\n#         weight_matrix = getattr(layer, f'{name}_weight')\n#         mask = torch.ones_like(weight_matrix.data)\n#         layer.register_buffer(f'{name}_pruning_mask', mask)\n        \n# x_model = pruned_model\n# x_model\n\n# def initialize_masks_for_model(model):\n#     for name, module in model.named_modules():\n#         # Check if the module is an MLP layer\n#         if isinstance(module, torch.nn.Linear):\n#             initialize_mask_MLP(module)\n#         # Check if the module is a self-attention layer\n#         # This is a simplified check; you may need a more specific condition based on your model's architecture\n#         elif hasattr(module, 'query_weight') and hasattr(module, 'key_weight') and hasattr(module, 'value_weight'):\n#             initialize_mask_attn_layer(module)\n            \n            \n# initialize_masks_for_model(pruned_model)\n\n# def check_and_view_masks(model):\n#     for name, module in model.named_modules():\n#         # Check for MLP layer masks\n#         if hasattr(module, 'pruning_mask'):\n#             print(f\"{name} has a pruning mask.\")\n#             print(module.pruning_mask)\n#         # Check for self-attention layer masks\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 print(f\"{name} has a {mask_name}.\")\n#                 print(getattr(module, mask_name))\n\n# # # Call this function with your model\n# check_and_view_masks(pruned_model)\n\n# def copy_weights_to_mask(layer):\n#     # Assuming the mask is already initialized and has the same shape as the layer's weights\n#     layer.pruning_mask.data = torch.clone(layer.weight.data)\n    \n# def apply_z_score_normalization(mask):\n#     mean = torch.mean(mask)\n#     std = torch.std(mask)\n#     normalized_mask = (mask - mean) / std\n#     return normalized_mask\n\n\n# def binarize_mask(mask, pruning_ratio):\n#     # Flatten the mask to simplify thresholding\n#     flat_mask = mask.view(-1)\n#     # Calculate the number of weights to keep\n#     num_weights_to_keep = int((1 - pruning_ratio) * flat_mask.numel())\n#     # Use torch.topk to get the threshold value\n#     threshold_value, _ = torch.topk(flat_mask.abs(), num_weights_to_keep, largest=True)\n#     min_value_to_keep = threshold_value[-1]\n#     # Binarize the mask\n#     binarized_mask = torch.where(flat_mask.abs() >= min_value_to_keep, torch.tensor(1.0, device=mask.device), torch.tensor(0.0, device=mask.device))\n#     return binarized_mask.view_as(mask)\n\n\n# class AdaptivePruning:\n#     def __init__(self, prev_val_acc, initial_pruning_rate=0.1, delta=0.01, performance_threshold=0.02):\n#         self.pruning_rate = initial_pruning_rate\n#         self.delta = delta  # Increment/decrement step for the pruning rate\n#         self.performance_threshold = performance_threshold  # Minimum acceptable change in validation accuracy\n#         self.previous_accuracy = prev_val_acc  # Placeholder for the last recorded accuracy\n\n#     def adjust_pruning_rate(self, current_accuracy):\n#         \"\"\"\n#         Adjusts the pruning rate based on the change in accuracy.\n        \n#         :param current_accuracy: The current accuracy of the model on the validation set.\n#         \"\"\"\n#         accuracy_change = current_accuracy - self.previous_accuracy\n\n#         # If performance drops significantly, decrease pruning rate\n#         if accuracy_change < -self.performance_threshold:\n#             self.pruning_rate = max(self.pruning_rate - self.delta, 0)  # Ensure pruning rate doesn't go negative\n#         # If performance improves or remains stable, consider increasing the pruning rate\n#         elif accuracy_change >= self.performance_threshold:\n#             self.pruning_rate = min(self.pruning_rate + self.delta, 1)  # Ensure pruning rate doesn't exceed 1\n\n#         self.previous_accuracy = current_accuracy  # Update the previous accuracy\n#         return self.pruning_rate\n\n    \n# import torch\n# import torch.nn.functional as F\n\n# def combined_loss(output, target, model, lambda_sparsity=1e-4):\n#     \"\"\"\n#     Calculate combined loss = CrossEntropyLoss + lambda * SparsityLoss\n    \n#     :param output: Tensor, model output logits\n#     :param target: Tensor, ground truth labels\n#     :param model: PyTorch model, to calculate sparsity loss over its parameters\n#     :param lambda_sparsity: float, weighting factor for sparsity loss\n#     :return: combined loss value\n#     \"\"\"\n#     # Cross Entropy Loss\n#     ce_loss = F.cross_entropy(output, target)\n    \n#     # Sparsity Loss (L1 norm of model weights)\n#     sparsity_loss = 0\n#     for param in model.parameters():\n#         sparsity_loss += torch.sum(torch.abs(param))\n    \n#     # Combined Loss\n#     combined_loss = ce_loss + lambda_sparsity * sparsity_loss\n    \n#     return combined_loss\n\n# def update_pruning_rate(epoch, max_epoch, initial_rate, final_rate):\n#     \"\"\"\n#     Updates the pruning rate over time, increasing from an initial rate to a final rate linearly over the epochs.\n\n#     Args:\n#         epoch (int): Current epoch number.\n#         max_epoch (int): Total number of epochs for training.\n#         initial_rate (float): Initial pruning rate at the beginning of training.\n#         final_rate (float): Final pruning rate by the end of training.\n    \n#     Returns:\n#         float: The updated pruning rate for the current epoch.\n#     \"\"\"\n#     # Linear scheduling from initial_rate to final_rate\n#     current_rate = initial_rate + (final_rate - initial_rate) * (epoch / max_epoch)\n#     return current_rate\n\n# def apply_pruning(model):\n#     for name, module in model.named_modules():\n#         # Check for MLP layer masks\n#         if hasattr(module, 'pruning_mask'):\n#             print(f\"{name} has a pruning mask.\")\n#             module.weight.data = module.weight.data * module.pruning_mask\n#         # Check for self-attention layer masks\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 print(f\"{name} has a {mask_name}.\")\n#                 module.weight.data = module.weight.data * module.pruning_mask\n\n# import inspect\n# from transformers import DeiTForImageClassificationWithTeacher\n\n# # Attempt to print the forward method source code\n# print(inspect.getsource(DeiTForImageClassificationWithTeacher.forward))\n\n\n# import torchvision.utils as utils\n# class_labels = train_dataset.classes\n# class_labels\n\n# for data, targets in train_loader:\n#     print(f'{data.shape} {targets.shape}')\n    \n#     num_images_to_display = 4\n#     grid_images = utils.make_grid(data[:num_images_to_display], nrow=num_images_to_display)\n\n#     # Move the tensor to CPU and convert it to a NumPy array for visualization\n#     grid_images = grid_images.cpu().numpy().transpose((1, 2, 0))\n#     print([class_labels[x] for x in targets[:num_images_to_display]])\n#     # Display the images\n#     plt.imshow(grid_images)\n#     plt.title('Batch of Images')\n#     plt.axis('off')\n#     plt.show()\n#     break\n    \n    \n    \n# import torch\n# from tqdm import tqdm\n\n# # Assuming you have defined train_loader, test_loader, model, optimizer, and device\n\n# def train_one_epoch(epoch, model, train_loader, optimizer, device):\n#     model.train()\n#     running_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n#     for batch_idx, (inputs, targets) in progress_bar:\n#         inputs, targets = inputs.to(device), targets.to(device)\n        \n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = combined_loss(outputs.logits, targets, model)  # Use the defined combined_loss function\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n#         _, predicted = outputs.logits.max(1)\n#         total += targets.size(0)\n#         correct += predicted.eq(targets).sum().item()\n\n#         progress_bar.set_description(f'Epoch {epoch} Loss: {running_loss/(batch_idx+1):.3f} Acc: {100.*correct/total:.3f}%')\n    \n#     return running_loss / len(train_loader), 100.*correct / total\n\n# def validate(model, test_loader, device):\n#     model.eval()\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for inputs, targets in test_loader:\n#             inputs, targets = inputs.to(device), targets.to(device)\n#             outputs = model(inputs)\n#             _, predicted = outputs.logits.max(1)\n#             total += targets.size(0)\n#             correct += predicted.eq(targets).sum().item()\n    \n#     acc = 100.*correct / total\n#     print(f'Validation Accuracy: {acc:.3f}%')\n#     return acc\n\n# def unified_pruning(epoch, model, train_loader, optimizer, device, initial_pruning_rate, performance_threshold, max_epoch):\n    \n#     for name, module in model.named_modules():\n#         # Check if the module is an MLP layer or a self-attention layer\n#         if isinstance(module, torch.nn.Linear) or \\\n#         (hasattr(module, 'query_weight') and hasattr(module, 'key_weight') and hasattr(module, 'value_weight')):\n#             copy_weights_to_mask(module)\n   \n    \n#     for name, module in model.named_modules():\n    \n#         if hasattr(module, 'pruning_mask'):\n#             module.pruning_mask.data = apply_z_score_normalization(module.pruning_mask.data)\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 mask = getattr(module, mask_name)\n#                 setattr(module, mask_name, apply_z_score_normalization(mask))\n                \n#     current_accuracy = validate(model, test_loader, device) \n#     adaptive_pruning = AdaptivePruning(prev_val_acc=current_accuracy, initial_pruning_rate=initial_pruning_rate, delta=0.01, performance_threshold=performance_threshold)\n#     current_pruning_rate = adaptive_pruning.adjust_pruning_rate(current_accuracy)\n    \n#     for name, module in model.named_modules():\n        \n#         if hasattr(module, 'pruning_mask'):\n#             module.pruning_mask.data = binarize_mask(module.pruning_mask.data, current_pruning_rate)\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 mask = getattr(module, mask_name)\n#                 setattr(module, mask_name, binarize_mask(mask, current_pruning_rate))\n#     apply_pruning(model)\n#     train_loss, train_acc = train_one_epoch(epoch, model, train_loader, optimizer, device)\n#     return train_loss, train_acc\n# # epoch_loss, epoch_acc = unified_pruning(pruned_model, current_epoch, total_epochs, train_loader, optimizer, device, initial_rate, final_rate)\n\n\n# def progressive_pruning(model, epoch, max_epoch, train_loader, optimizer, device, initial_pruning_rate, final_pruning_rate):\n\n#     for name, module in model.named_modules():\n#         if isinstance(module, torch.nn.Linear) or \\\n#         (hasattr(module, 'query_weight') and hasattr(module, 'key_weight') and hasattr(module, 'value_weight')):\n#             copy_weights_to_mask(module)\n    \n#     for name, module in model.named_modules():\n#         if hasattr(module, 'pruning_mask'):\n#             module.pruning_mask.data = apply_z_score_normalization(module.pruning_mask)\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 getattr(module, mask_name).data = apply_z_score_normalization(getattr(module, mask_name))\n                \n#     pruning_rate = update_pruning_rate(epoch, max_epoch, initial_pruning_rate, final_pruning_rate)\n   \n#     for name, module in model.named_modules():\n        \n#         if hasattr(module, 'pruning_mask'):\n#             module.pruning_mask.data = binarize_mask(module.pruning_mask.data, pruning_rate)\n#         for attn_part in ['query', 'key', 'value']:\n#             mask_name = f'{attn_part}_pruning_mask'\n#             if hasattr(module, mask_name):\n#                 mask = getattr(module, mask_name)\n#                 setattr(module, mask_name, binarize_mask(mask, pruning_rate))\n                \n#     apply_pruning(model)\n#     train_loss, train_acc = train_one_epoch(epoch, model, train_loader, optimizer, device)\n    \n#     return train_loss, train_acc\n\n\n# # Hyperparameters and settings\n# initial_pruning_rate = 0.1  # Example initial pruning rate\n# final_pruning_rate = 0.5   # Example final pruning rate for unified pruning\n# performance_threshold = 0.02  # Example performance threshold for progressive pruning\n# lambda_sparsity = 1e-4  # Sparsity weighting factor for combined loss\n# optimizer = optim.Adam(pruned_model.parameters(), lr=0.001)\n# max_epoch = 16\n\n# #Initiating Training\n# for epoch in range(1, 17):  # 1 to 16 epochs\n#     if epoch in [4, 8, 12]:\n#         # Apply Unified Pruning\n#         print(f\"Epoch {epoch}: Applying Unified Pruning\")\n#         unified_pruning(epoch, pruned_model, train_loader, optimizer, device, initial_pruning_rate, final_pruning_rate, 16)\n#     elif epoch in [15, 16]:\n#         # Train the model normally\n#         print(f\"Epoch {epoch}: Normal Training\")\n#         train_loss, train_acc = train_one_epoch(epoch, pruned_model, train_loader, optimizer, device)\n#         print(f\"Training Loss: {train_loss}, Training Accuracy: {train_acc}\")\n#     else:\n#         # Apply Progressive Pruning\n#         print(f\"Epoch {epoch}: Applying Progressive Pruning\")\n#         progressive_pruning(pruned_model, epoch, max_epoch, train_loader, optimizer, device, initial_pruning_rate, final_pruning_rate)\n\n#     # Optional: Perform validation after each epoch to monitor progress\n#     val_acc = validate(pruned_model, test_loader, device)\n#     print(f\"Validation Accuracy: {val_acc}%\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_and_view_masks(pruned_model)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_mask_binarization(pruned_model):\n    for name, module in pruned_model.named_modules():\n        if hasattr(module, 'pruning_mask'):\n            mask = module.pruning_mask\n            if not torch.all((mask == 0) | (mask == 1)):\n                print(f\"Mask in {name} is not binary.\")\n                return False\n    print(\"All masks are binary.\")\n    return True\n\ndef verify_magnitude_based_ranking(pruned_model):\n    for name, module in pruned_model.named_modules():\n        if hasattr(module, 'pruning_mask') and hasattr(module, 'weight'):\n            weights = module.weight.data.abs()\n            mask = module.pruning_mask\n            pruned_weights = weights * (1 - mask)\n            if pruned_weights.max() > weights.min():\n                print(f\"Ranking issue in {name}: pruned max > unpruned min.\")\n                return False\n    print(\"Magnitude-based ranking verified.\")\n    return True\n\ndef assess_model_pruning(pruned_model, expected_pruning_rate):\n    total_weights = sum(p.numel() for p in pruned_model.parameters())\n    total_non_zero = sum(p.count_nonzero().item() for p in pruned_model.parameters() if p.requires_grad)\n    actual_pruning_rate = (total_weights - total_non_zero) / total_weights\n    print(f\"Actual pruning rate: {actual_pruning_rate:.2f}, Expected: {expected_pruning_rate}\")\n    print(f\"Total weights: {total_weights}, Total non-zero (active) weights: {total_non_zero}\")\n    return abs(actual_pruning_rate - expected_pruning_rate) < 0.01\n\nexpected_pruning_rate = 0.5\n\nimport torch\nimport numpy as np\n\n# Function to check Z-score normalization\ndef check_z_score_normalization(model):\n    z_score_issues = []\n    for name, module in model.named_modules():\n        if hasattr(module, 'pruning_mask'):\n            mean = torch.mean(module.pruning_mask).item()\n            std = torch.std(module.pruning_mask).item()\n            if not np.isclose(mean, 0, atol=0.1) or not np.isclose(std, 1, atol=0.1):\n                z_score_issues.append(name)\n    if z_score_issues:\n        print(f\"Z-score normalization issue in: {z_score_issues}\")\n    else:\n        print(\"All masks correctly normalized with Z-score.\")\n\n# Function to verify pruning operations\ndef verify_pruning_operations(model):\n    for epoch in [4, 8, 12]:\n        print(f\"Verifying unified pruning for epoch {epoch}...\")\n    for epoch in set(range(1, 17)) - {4, 8, 12, 15, 16}:\n        print(f\"Verifying progressive pruning for epoch {epoch}...\")\n    print(\"Verifying normal training for epochs 15 and 16...\")\n\n# Function to check pruning rate adjustments\ndef check_pruning_rate_adjustments(model, initial_pruning_rate, final_pruning_rate):\n    # This is a conceptual check; implement logic based on how you adjust pruning rates in your model\n    print(f\"Initial pruning rate set to: {initial_pruning_rate}, aiming for final rate: {final_pruning_rate}\")\n    print(\"Assuming dynamic adjustments are made based on performance thresholds.\")\n\n# Integrate debugging checks into training loop\ndef integrated_debugging_checks():\n    check_z_score_normalization(pruned_model)\n    verify_pruning_operations(pruned_model)\n    check_pruning_rate_adjustments(pruned_model, initial_pruning_rate, final_pruning_rate)\n    \n\n# Call this function at the beginning or end of your training loop, or wherever appropriate\nintegrated_debugging_checks()\n\n# Run checks\ncheck_mask_binarization(pruned_model)\nverify_magnitude_based_ranking(pruned_model)\nif assess_model_pruning(pruned_model, expected_pruning_rate):\n    print(\"Model pruning is within expected bounds.\")\nelse:\n    print(\"Model pruning is not within expected bounds.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_model_parameters(model):\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            non_zero_count = torch.count_nonzero(param).item()\n            total_elements = param.numel()\n            sparsity = 1 - (non_zero_count / total_elements)\n            print(f\"{name}: shape = {param.shape}, non-zero elements = {non_zero_count}/{total_elements}, sparsity = {sparsity:.2f}\")\n\n\ndisplay_model_parameters(pruned_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_pruning_effectiveness(pruned_model, final_pruning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}